{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m15k4Zxd0wNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0e523352-7995-48f1-d98a-5fe8d3324b56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn"
      ],
      "metadata": {
        "id": "JsjV5g1JJTB1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Data(preparing and loading)\n"
      ],
      "metadata": {
        "id": "LfqxUpyIJ8UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create parameters\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "# create data\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "X[:10], y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WodHC33jJsCc",
        "outputId": "9b4d5839-81b0-461d-a18e-dc03f2f049da"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]), tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting data into training and test sets"
      ],
      "metadata": {
        "id": "YnwC_679LFHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = int(0.8 * len(X))\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-a0N0F_KzH7",
        "outputId": "3f95b463-4fbe-4480-9c01-5f0a0c494476"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Build model"
      ],
      "metadata": {
        "id": "0-kc8oN4Lnow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,\n",
        "                                            requires_grad=True,\n",
        "                                            dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                                         requires_grad=True,\n",
        "                                         dtype=torch.float))\n",
        "  def forward(self, x:torch.Tensor):\n",
        "    return self.weights * x + self.bias"
      ],
      "metadata": {
        "id": "rbJPn3IqLgJK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set manual seed \n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an instance of our model\n",
        "model_0 = LinearRegression()\n",
        "\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX5vD6sWMsBI",
        "outputId": "f36eb8d8-2192-45f2-d1f0-651468cfb484"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True), Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMjUqxKqNqrG",
        "outputId": "dd9bd596-30df-411b-ac7f-0f9e62de2ee6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CtyNUDgUNZG",
        "outputId": "fd7af55d-88e7-4892-f6a5-1e6f64943117"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eWLqS7EUa4V",
        "outputId": "f7ca6e8f-51dd-4376-81b4-210306f771a7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8600],\n",
              "        [0.8740],\n",
              "        [0.8880],\n",
              "        [0.9020],\n",
              "        [0.9160],\n",
              "        [0.9300],\n",
              "        [0.9440],\n",
              "        [0.9580],\n",
              "        [0.9720],\n",
              "        [0.9860]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "usvk4-MJOPY4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Model Training"
      ],
      "metadata": {
        "id": "qLS_ij_jmekU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "\n",
        "# Loop through data\n",
        "for epoch in range(epochs):\n",
        "  # Set training mode\n",
        "  model_0.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_pred = model_0(X_train)\n",
        "\n",
        "  # 2. Loss function\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "  # 3. optimizer(zero_grad)(Get to zero)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Backword propagation\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizers.step\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing \n",
        "  \n",
        "  # Set model evaluation mode\n",
        "  model_0.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    # 1. forward pass\n",
        "    test_pred = model_0(X_test)\n",
        "\n",
        "    # 2. Calculate loss\n",
        "    test_loss = loss_fn(test_pred, y_test.type(torch.float))\n",
        "\n",
        "    if epochs % 10 == 0:\n",
        "      epoch_count.append(epoch)\n",
        "      train_loss_values.append(loss.detach().numpy())\n",
        "      test_loss_values.append(test_loss.detach().numpy())\n",
        "      print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7rKs-2Dcvgh",
        "outputId": "ada29be9-bc71-4aaf-fc98-d1e1359ffcf7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \n",
            "Epoch: 1 | MAE Train Loss: 0.3013603389263153 | MAE Test Loss: 0.4675942063331604 \n",
            "Epoch: 2 | MAE Train Loss: 0.28983935713768005 | MAE Test Loss: 0.4541231691837311 \n",
            "Epoch: 3 | MAE Train Loss: 0.2783183455467224 | MAE Test Loss: 0.44065219163894653 \n",
            "Epoch: 4 | MAE Train Loss: 0.26679736375808716 | MAE Test Loss: 0.4271811842918396 \n",
            "Epoch: 5 | MAE Train Loss: 0.2552763521671295 | MAE Test Loss: 0.41371020674705505 \n",
            "Epoch: 6 | MAE Train Loss: 0.24375534057617188 | MAE Test Loss: 0.40023916959762573 \n",
            "Epoch: 7 | MAE Train Loss: 0.23223432898521423 | MAE Test Loss: 0.3867681920528412 \n",
            "Epoch: 8 | MAE Train Loss: 0.22071333229541779 | MAE Test Loss: 0.37329721450805664 \n",
            "Epoch: 9 | MAE Train Loss: 0.20919232070446014 | MAE Test Loss: 0.3598262071609497 \n",
            "Epoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 \n",
            "Epoch: 11 | MAE Train Loss: 0.18615034222602844 | MAE Test Loss: 0.3328842222690582 \n",
            "Epoch: 12 | MAE Train Loss: 0.1746293306350708 | MAE Test Loss: 0.3194132149219513 \n",
            "Epoch: 13 | MAE Train Loss: 0.16310831904411316 | MAE Test Loss: 0.30594223737716675 \n",
            "Epoch: 14 | MAE Train Loss: 0.1515873372554779 | MAE Test Loss: 0.2924712300300598 \n",
            "Epoch: 15 | MAE Train Loss: 0.14006635546684265 | MAE Test Loss: 0.27900025248527527 \n",
            "Epoch: 16 | MAE Train Loss: 0.1285453587770462 | MAE Test Loss: 0.2655292749404907 \n",
            "Epoch: 17 | MAE Train Loss: 0.11702437698841095 | MAE Test Loss: 0.2520582973957062 \n",
            "Epoch: 18 | MAE Train Loss: 0.1060912236571312 | MAE Test Loss: 0.2395961582660675 \n",
            "Epoch: 19 | MAE Train Loss: 0.09681284427642822 | MAE Test Loss: 0.22817862033843994 \n",
            "Epoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \n",
            "Epoch: 21 | MAE Train Loss: 0.08227583020925522 | MAE Test Loss: 0.2069590985774994 \n",
            "Epoch: 22 | MAE Train Loss: 0.07638873159885406 | MAE Test Loss: 0.19773726165294647 \n",
            "Epoch: 23 | MAE Train Loss: 0.07160007208585739 | MAE Test Loss: 0.1890866756439209 \n",
            "Epoch: 24 | MAE Train Loss: 0.06747635453939438 | MAE Test Loss: 0.18101617693901062 \n",
            "Epoch: 25 | MAE Train Loss: 0.06395438313484192 | MAE Test Loss: 0.17353470623493195 \n",
            "Epoch: 26 | MAE Train Loss: 0.06097004935145378 | MAE Test Loss: 0.16665108501911163 \n",
            "Epoch: 27 | MAE Train Loss: 0.05845819041132927 | MAE Test Loss: 0.16037428379058838 \n",
            "Epoch: 28 | MAE Train Loss: 0.05635259300470352 | MAE Test Loss: 0.15471318364143372 \n",
            "Epoch: 29 | MAE Train Loss: 0.0545857772231102 | MAE Test Loss: 0.14967669546604156 \n",
            "Epoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 \n",
            "Epoch: 31 | MAE Train Loss: 0.05181945487856865 | MAE Test Loss: 0.14023718237876892 \n",
            "Epoch: 32 | MAE Train Loss: 0.05069301277399063 | MAE Test Loss: 0.13647659122943878 \n",
            "Epoch: 33 | MAE Train Loss: 0.0498228520154953 | MAE Test Loss: 0.13271598517894745 \n",
            "Epoch: 34 | MAE Train Loss: 0.04895269125699997 | MAE Test Loss: 0.12895536422729492 \n",
            "Epoch: 35 | MAE Train Loss: 0.04819351062178612 | MAE Test Loss: 0.12584610283374786 \n",
            "Epoch: 36 | MAE Train Loss: 0.047531817108392715 | MAE Test Loss: 0.12273679673671722 \n",
            "Epoch: 37 | MAE Train Loss: 0.04692792519927025 | MAE Test Loss: 0.1202877014875412 \n",
            "Epoch: 38 | MAE Train Loss: 0.04642331600189209 | MAE Test Loss: 0.11783860623836517 \n",
            "Epoch: 39 | MAE Train Loss: 0.04591871052980423 | MAE Test Loss: 0.11538954079151154 \n",
            "Epoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \n",
            "Epoch: 41 | MAE Train Loss: 0.04503796249628067 | MAE Test Loss: 0.11182951927185059 \n",
            "Epoch: 42 | MAE Train Loss: 0.04463795945048332 | MAE Test Loss: 0.1100495308637619 \n",
            "Epoch: 43 | MAE Train Loss: 0.04423796385526657 | MAE Test Loss: 0.10826952755451202 \n",
            "Epoch: 44 | MAE Train Loss: 0.04383796453475952 | MAE Test Loss: 0.10648951679468155 \n",
            "Epoch: 45 | MAE Train Loss: 0.04343796148896217 | MAE Test Loss: 0.10470950603485107 \n",
            "Epoch: 46 | MAE Train Loss: 0.043074630200862885 | MAE Test Loss: 0.10360751301050186 \n",
            "Epoch: 47 | MAE Train Loss: 0.04272563382983208 | MAE Test Loss: 0.10250549018383026 \n",
            "Epoch: 48 | MAE Train Loss: 0.04237663000822067 | MAE Test Loss: 0.10140349715948105 \n",
            "Epoch: 49 | MAE Train Loss: 0.04202762991189957 | MAE Test Loss: 0.10030148178339005 \n",
            "Epoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 \n",
            "Epoch: 51 | MAE Train Loss: 0.04132963344454765 | MAE Test Loss: 0.09809747338294983 \n",
            "Epoch: 52 | MAE Train Loss: 0.04098063334822655 | MAE Test Loss: 0.09699545800685883 \n",
            "Epoch: 53 | MAE Train Loss: 0.04063162952661514 | MAE Test Loss: 0.09589345753192902 \n",
            "Epoch: 54 | MAE Train Loss: 0.040282636880874634 | MAE Test Loss: 0.0947914719581604 \n",
            "Epoch: 55 | MAE Train Loss: 0.039933640509843826 | MAE Test Loss: 0.09368947893381119 \n",
            "Epoch: 56 | MAE Train Loss: 0.03958464413881302 | MAE Test Loss: 0.09258746355772018 \n",
            "Epoch: 57 | MAE Train Loss: 0.03923564404249191 | MAE Test Loss: 0.09148545563220978 \n",
            "Epoch: 58 | MAE Train Loss: 0.03888664394617081 | MAE Test Loss: 0.09038344770669937 \n",
            "Epoch: 59 | MAE Train Loss: 0.0385376438498497 | MAE Test Loss: 0.08928143978118896 \n",
            "Epoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \n",
            "Epoch: 61 | MAE Train Loss: 0.03785243630409241 | MAE Test Loss: 0.08776430785655975 \n",
            "Epoch: 62 | MAE Train Loss: 0.0375034399330616 | MAE Test Loss: 0.08666229248046875 \n",
            "Epoch: 63 | MAE Train Loss: 0.037164121866226196 | MAE Test Loss: 0.08624717593193054 \n",
            "Epoch: 64 | MAE Train Loss: 0.03681822493672371 | MAE Test Loss: 0.08514519035816193 \n",
            "Epoch: 65 | MAE Train Loss: 0.03647511452436447 | MAE Test Loss: 0.08473004400730133 \n",
            "Epoch: 66 | MAE Train Loss: 0.03613303601741791 | MAE Test Loss: 0.08362803608179092 \n",
            "Epoch: 67 | MAE Train Loss: 0.03578609973192215 | MAE Test Loss: 0.08321291208267212 \n",
            "Epoch: 68 | MAE Train Loss: 0.03544783592224121 | MAE Test Loss: 0.08211090415716171 \n",
            "Epoch: 69 | MAE Train Loss: 0.035098835825920105 | MAE Test Loss: 0.0810088962316513 \n",
            "Epoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 \n",
            "Epoch: 71 | MAE Train Loss: 0.03441363573074341 | MAE Test Loss: 0.0794917643070221 \n",
            "Epoch: 72 | MAE Train Loss: 0.03407188132405281 | MAE Test Loss: 0.07907666265964508 \n",
            "Epoch: 73 | MAE Train Loss: 0.03372843936085701 | MAE Test Loss: 0.07797462493181229 \n",
            "Epoch: 74 | MAE Train Loss: 0.03338287025690079 | MAE Test Loss: 0.07755951583385468 \n",
            "Epoch: 75 | MAE Train Loss: 0.033043231815099716 | MAE Test Loss: 0.07645749300718307 \n",
            "Epoch: 76 | MAE Train Loss: 0.03269423171877861 | MAE Test Loss: 0.07535548508167267 \n",
            "Epoch: 77 | MAE Train Loss: 0.032357655465602875 | MAE Test Loss: 0.07494036853313446 \n",
            "Epoch: 78 | MAE Train Loss: 0.03200903534889221 | MAE Test Loss: 0.07383836805820465 \n",
            "Epoch: 79 | MAE Train Loss: 0.03166864812374115 | MAE Test Loss: 0.07342323660850525 \n",
            "Epoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \n",
            "Epoch: 81 | MAE Train Loss: 0.030979642644524574 | MAE Test Loss: 0.07190609723329544 \n",
            "Epoch: 82 | MAE Train Loss: 0.030638623982667923 | MAE Test Loss: 0.07080408930778503 \n",
            "Epoch: 83 | MAE Train Loss: 0.0302906334400177 | MAE Test Loss: 0.07038896530866623 \n",
            "Epoch: 84 | MAE Train Loss: 0.029953425750136375 | MAE Test Loss: 0.06928696483373642 \n",
            "Epoch: 85 | MAE Train Loss: 0.02960442565381527 | MAE Test Loss: 0.06818496435880661 \n",
            "Epoch: 86 | MAE Train Loss: 0.029265418648719788 | MAE Test Loss: 0.0677698403596878 \n",
            "Epoch: 87 | MAE Train Loss: 0.028919223695993423 | MAE Test Loss: 0.0666678324341774 \n",
            "Epoch: 88 | MAE Train Loss: 0.028576409444212914 | MAE Test Loss: 0.066252700984478 \n",
            "Epoch: 89 | MAE Train Loss: 0.028234025463461876 | MAE Test Loss: 0.06515069305896759 \n",
            "Epoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819 \n",
            "Epoch: 91 | MAE Train Loss: 0.02754882536828518 | MAE Test Loss: 0.06363357603549957 \n",
            "Epoch: 92 | MAE Train Loss: 0.027199819684028625 | MAE Test Loss: 0.06253156810998917 \n",
            "Epoch: 93 | MAE Train Loss: 0.026862185448408127 | MAE Test Loss: 0.062116436660289764 \n",
            "Epoch: 94 | MAE Train Loss: 0.02651461586356163 | MAE Test Loss: 0.061014432460069656 \n",
            "Epoch: 95 | MAE Train Loss: 0.026173178106546402 | MAE Test Loss: 0.06059930846095085 \n",
            "Epoch: 96 | MAE Train Loss: 0.025829419493675232 | MAE Test Loss: 0.05949730426073074 \n",
            "Epoch: 97 | MAE Train Loss: 0.02548416517674923 | MAE Test Loss: 0.05908216908574104 \n",
            "Epoch: 98 | MAE Train Loss: 0.025144213810563087 | MAE Test Loss: 0.057980168610811234 \n",
            "Epoch: 99 | MAE Train Loss: 0.02479521557688713 | MAE Test Loss: 0.05687814950942993 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = 100\n",
        "\n",
        "# epoch_count = []\n",
        "# train_loss = []\n",
        "# test_loss = []\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "\n",
        "#   # Set training mode\n",
        "#   model_0.train()\n",
        "\n",
        "#   # Forward pass\n",
        "#   y_pred = model_0(X_train)\n",
        "\n",
        "#   # Loss function\n",
        "#   loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "#   # optimizer to go zero_grad()\n",
        "#   optimizer.zero_grad()\n",
        "\n",
        "#   # loss backward\n",
        "#   loss.backward()\n",
        "\n",
        "#   # optimizer step\n",
        "#   optimizer.step()\n",
        "\n",
        "#   # Set testing model\n",
        "#   model_0.eval()\n",
        "\n",
        "#   with torch.inference_mode():\n",
        "#     test_pred = model_0(X_test)\n",
        "\n",
        "#     test_loss = loss_fn(test_pred, y_test)\n",
        "\n",
        "#     if epochs % 10 == 0:\n",
        "#       epoch_count.append(epoch)\n",
        "#       train_loss.append(loss.detach().numpy())\n",
        "#       test_loss.append(test_loss.detach().numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "2C_tOMFBgMlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Make Predictions"
      ],
      "metadata": {
        "id": "geIWe9lrmmk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_35L2j8LlySW",
        "outputId": "7b147e4a-1747-4071-e930-766387d30e89"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8141],\n",
              "        [0.8256],\n",
              "        [0.8372],\n",
              "        [0.8488],\n",
              "        [0.8603],\n",
              "        [0.8719],\n",
              "        [0.8835],\n",
              "        [0.8950],\n",
              "        [0.9066],\n",
              "        [0.9182]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Saving and loading a PyTorch  model"
      ],
      "metadata": {
        "id": "UMN8IG7Kmr5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path \n",
        "\n",
        "# Create models directory\n",
        "MODEL_PATH = Path('models')\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = '01_pytorch_workflow_model_0.pth'\n",
        "MODEL_SAVE_PATH = MODEL_PATH/ MODEL_NAME\n",
        "\n",
        "print(f'Saving model to:{MODEL_SAVE_PATH}')\n",
        "torch.save(obj=model_0.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIDAFDEzl55s",
        "outputId": "bc2ce012-5159-46ce-c85d-1e11b54f0729"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model to:models/01_pytorch_workflow_model_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "\n",
        "# model = Path('model')\n",
        "# model.mkdir(parent=True, exist_ok=True)\n",
        "\n",
        "# model_path = 'pytorch_1.pth'\n",
        "# model_save_path = model/model_path\n",
        "\n",
        "# torch.save(model_0.state_dict(), model_save_path)"
      ],
      "metadata": {
        "id": "H1RDxG1jnakP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model = LinearRegression()\n",
        "\n",
        "load_model.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5vRsXECoXS7",
        "outputId": "eb5476de-7e02-4361-817c-aa2f26ee979f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_model.eval()\n",
        "with torch.inference_mode():\n",
        "  loaded_preds = load_model(X_test)\n",
        "loaded_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrhGHh0ooeNi",
        "outputId": "4efdb9da-7638-4887-bfff-3fcaef4612d0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8141],\n",
              "        [0.8256],\n",
              "        [0.8372],\n",
              "        [0.8488],\n",
              "        [0.8603],\n",
              "        [0.8719],\n",
              "        [0.8835],\n",
              "        [0.8950],\n",
              "        [0.9066],\n",
              "        [0.9182]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISES"
      ],
      "metadata": {
        "id": "RDws5OIqphUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Create a straight line dataset using the linear regression formula (weight * X + bias).\n",
        "* Set weight=0.3 and bias=0.9 \n",
        "there should be at least 100 datapoints total.\n",
        "* Split the data into 80% training, 20% testing.\n",
        "* Plot the training and testing data so it becomes visual."
      ],
      "metadata": {
        "id": "lY_3jKMppk8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 0.3\n",
        "b = 0.9\n",
        "\n",
        "start = 0.0\n",
        "end = 100.0\n",
        "step = 1.0"
      ],
      "metadata": {
        "id": "AlcL1q8wo0qR"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(start, end, step)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvebSPRLqDhM",
        "outputId": "d3260327-6a29-446c-9d22-8a74e067cc34"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
              "        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\n",
              "        28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41.,\n",
              "        42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55.,\n",
              "        56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69.,\n",
              "        70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83.,\n",
              "        84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97.,\n",
              "        98., 99.])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKX5k_ArqGzc",
        "outputId": "4b13bf05-f290-48fb-f1c9-790191ec1f3f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = w*x + b\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkh2ZO-LqKgC",
        "outputId": "772e2d61-ba43-4d7a-9f27-b1b61e84ed80"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.9000,  1.2000,  1.5000,  1.8000,  2.1000,  2.4000,  2.7000,  3.0000,\n",
              "         3.3000,  3.6000,  3.9000,  4.2000,  4.5000,  4.8000,  5.1000,  5.4000,\n",
              "         5.7000,  6.0000,  6.3000,  6.6000,  6.9000,  7.2000,  7.5000,  7.8000,\n",
              "         8.1000,  8.4000,  8.7000,  9.0000,  9.3000,  9.6000,  9.9000, 10.2000,\n",
              "        10.5000, 10.8000, 11.1000, 11.4000, 11.7000, 12.0000, 12.3000, 12.6000,\n",
              "        12.9000, 13.2000, 13.5000, 13.8000, 14.1000, 14.4000, 14.7000, 15.0000,\n",
              "        15.3000, 15.6000, 15.9000, 16.2000, 16.5000, 16.8000, 17.1000, 17.4000,\n",
              "        17.7000, 18.0000, 18.3000, 18.6000, 18.9000, 19.2000, 19.5000, 19.8000,\n",
              "        20.1000, 20.4000, 20.7000, 21.0000, 21.3000, 21.6000, 21.9000, 22.2000,\n",
              "        22.5000, 22.8000, 23.1000, 23.4000, 23.7000, 24.0000, 24.3000, 24.6000,\n",
              "        24.9000, 25.2000, 25.5000, 25.8000, 26.1000, 26.4000, 26.7000, 27.0000,\n",
              "        27.3000, 27.6000, 27.9000, 28.2000, 28.5000, 28.8000, 29.1000, 29.4000,\n",
              "        29.7000, 30.0000, 30.3000, 30.6000])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6HRA2RsqWwi",
        "outputId": "69ef5468-d860-4b31-9920-3cd4e8fbbed3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DWWEuQOqYlH",
        "outputId": "f3d3e00c-4899-4fe4-aa39-f9eaddc1381a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = int(0.8*len(x))\n",
        "X_train, y_train = x[:train_split], y[:train_split]\n",
        "X_test, y_test = x[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test),len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1OdtItPqacR",
        "outputId": "74a531b5-0967-408c-cc5e-a29aa242d31a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 80, 20, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_predictions(train_data=X_train, \n",
        "                     train_labels=y_train, \n",
        "                     test_data=X_test, \n",
        "                     test_labels=y_test, \n",
        "                     predictions=None):\n",
        "  \"\"\"\n",
        "  Plots training data, test data and compares predictions.\n",
        "  \"\"\"\n",
        "  plt.figure(figsize=(10, 7))\n",
        "\n",
        "  # Plot training data in blue\n",
        "  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "  \n",
        "  # Plot test data in green\n",
        "  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    # Plot the predictions in red (predictions were made on the test data)\n",
        "    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "\n",
        "  # Show the legend\n",
        "  plt.legend(prop={\"size\": 14});"
      ],
      "metadata": {
        "id": "eMvA-WsBq4XI"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "rXNqxR1qq8TH",
        "outputId": "f287b0b4-5a9d-4f7b-b7c9-ee4d26958647"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Build a PyTorch model by subclassing nn.Module.\n",
        "* Inside should be a randomly initialized nn.Parameter() with requires_grad=True, one for weights and one for bias.\n",
        "* Implement the forward() method to compute the linear regression function you used to create the dataset in 1.\n",
        "* Once you've constructed the model, make an instance of it and check its state_dict().\n",
        "* Note: If you'd like to use nn.Linear() instead of nn.Parameter() you can."
      ],
      "metadata": {
        "id": "_v1vyN0GrMch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,\n",
        "                                           requires_grad=True,\n",
        "                                           dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                                         requires_grad=True,\n",
        "                                         dtype=torch.float))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.weights * x + self.bias"
      ],
      "metadata": {
        "id": "8nujCWqZrHR_"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "li = LinearRegression()\n",
        "li.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0dcf3IDsCNC",
        "outputId": "ff50e9ba-c930-4540-b246-7ec5e0e75f7a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(li.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MACsgnusLYx",
        "outputId": "49af543d-12dc-4543-ebcb-0878ae885b73"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True), Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Create a loss function and optimizer using nn.L1Loss() and torch.optim.SGD(params, lr) respectively.\n",
        "* Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2.\n",
        "* Write a training loop to perform the appropriate training steps for 300 epochs.\n",
        "* The training loop should test the model on the test dataset every 20 epochs."
      ],
      "metadata": {
        "id": "Fr8c91bxsg2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "# Optimizer\n",
        "optim = torch.optim.SGD(params=li.parameters(),\n",
        "                        lr=0.01)"
      ],
      "metadata": {
        "id": "iQjsLDmpsaDh"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 300\n",
        "\n",
        "epoch_count = []\n",
        "train_loss = []\n",
        "test_loss_values = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Set training model\n",
        "  li.train()\n",
        "\n",
        "  # Forward pass\n",
        "  y_pred = li(X_train)\n",
        "\n",
        "  # Loss fun\n",
        "  loss = loss_fn(y_pred, y_pred)\n",
        "\n",
        "  # optimizer(zero_grad)\n",
        "  optim.zero_grad()\n",
        "\n",
        "  # loss backward\n",
        "  loss.backward()\n",
        "\n",
        "  # optimizer step\n",
        "  optim.step()\n",
        "\n",
        "  # Testing mode\n",
        "  li.eval()\n",
        "  \n",
        "  with torch.inference_mode():\n",
        "    test_pred = li(X_test)\n",
        "    test_loss = loss_fn(test_pred, y_test)\n",
        "\n",
        "    if epochs % 20 == 0:\n",
        "      epoch_count.append(epoch)\n",
        "      train_loss.append(loss.detach().numpy())\n",
        "      test_loss_values.append(test_loss.detach().numpy())\n",
        "      print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo1pdboTtGYC",
        "outputId": "40c018e4-1465-4d32-a463-7312df1ba4e2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 1 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 2 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 3 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 4 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 5 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 6 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 7 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 8 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 9 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 10 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 11 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 12 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 13 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 14 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 15 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 16 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 17 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 18 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 19 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 20 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 21 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 22 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 23 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 24 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 25 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 26 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 27 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 28 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 29 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 30 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 31 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 32 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 33 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 34 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 35 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 36 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 37 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 38 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 39 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 40 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 41 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 42 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 43 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 44 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 45 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 46 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 47 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 48 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 49 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 50 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 51 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 52 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 53 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 54 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 55 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 56 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 57 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 58 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 59 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 60 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 61 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 62 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 63 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 64 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 65 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 66 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 67 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 68 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 69 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 70 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 71 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 72 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 73 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 74 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 75 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 76 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 77 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 78 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 79 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 80 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 81 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 82 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 83 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 84 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 85 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 86 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 87 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 88 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 89 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 90 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 91 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 92 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 93 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 94 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 95 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 96 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 97 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 98 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 99 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 100 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 101 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 102 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 103 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 104 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 105 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 106 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 107 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 108 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 109 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 110 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 111 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 112 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 113 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 114 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 115 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 116 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 117 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 118 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 119 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 120 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 121 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 122 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 123 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 124 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 125 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 126 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 127 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 128 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 129 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 130 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 131 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 132 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 133 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 134 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 135 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 136 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 137 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 138 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 139 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 140 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 141 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 142 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 143 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 144 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 145 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 146 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 147 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 148 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 149 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 150 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 151 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 152 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 153 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 154 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 155 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 156 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 157 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 158 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 159 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 160 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 161 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 162 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 163 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 164 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 165 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 166 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 167 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 168 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 169 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 170 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 171 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 172 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 173 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 174 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 175 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 176 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 177 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 178 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 179 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 180 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 181 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 182 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 183 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 184 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 185 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 186 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 187 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 188 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 189 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 190 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 191 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 192 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 193 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 194 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 195 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 196 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 197 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 198 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 199 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 200 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 201 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 202 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 203 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 204 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 205 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 206 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 207 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 208 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 209 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 210 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 211 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 212 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 213 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 214 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 215 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 216 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 217 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 218 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 219 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 220 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 221 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 222 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 223 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 224 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 225 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 226 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 227 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 228 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 229 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 230 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 231 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 232 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 233 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 234 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 235 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 236 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 237 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 238 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 239 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 240 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 241 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 242 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 243 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 244 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 245 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 246 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 247 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 248 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 249 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 250 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 251 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 252 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 253 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 254 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 255 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 256 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 257 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 258 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 259 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 260 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 261 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 262 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 263 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 264 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 265 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 266 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 267 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 268 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 269 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 270 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 271 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 272 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 273 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 274 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 275 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 276 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 277 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 278 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 279 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 280 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 281 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 282 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 283 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 284 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 285 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 286 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 287 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 288 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 289 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 290 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 291 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 292 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 293 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 294 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 295 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 296 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 297 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 298 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n",
            "Epoch: 299 | MAE Train Loss: 0.0 | MAE Test Loss: 2.512596607208252 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, train_loss, label=\"Train loss\")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "M7yf18UvuXAh",
        "outputId": "bc492b44-dcb5-4214-c721-c0dc137bb960"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fLm7gO_uk9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}